---
output: dynbenchmark::github_markdown_nested
---

# Metric conformity

Differences between two datasets should be reflected in certain changes in the metrics. This can be formalised in a set of rules, for example:

* If the position of some cells are different than in the reference, the score should decrease.
* If the topology of the network is different than that in the reference, the score should not be perfect.
* The more cells are filtered from the trajectory, the more the score should decrease.

Here, we assess whether metrics conforms such rules empirically:


```{r, results = "asis"}
dynbenchmark::render_scripts_documentation()
```

The results of this experiment are available [here](`r dynbenchmark::link_to_results()`).
